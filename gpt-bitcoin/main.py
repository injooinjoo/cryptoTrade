import logging
import os
import time
from typing import Dict, Any

import numpy as np
import pandas as pd
import schedule
from dotenv import load_dotenv

from api_client import UpbitClient, OpenAIClient, OrderManager, PositionManager
from auto_adjustment import AutoAdjustment, AnomalyDetector, MarketRegimeDetector
from backtesting import run_backtest
from config import load_config, setup_logging
from data_preparation import safe_fetch_multi_timeframe_data
from database import initialize_db, get_previous_decision, update_decision_accuracy, get_recent_decisions
from discord_notifier import send_discord_message
from ml_models import MLPredictor, RLAgent
from performance_monitor import PerformanceMonitor
from trading_logic import analyze_data_with_gpt4, execute_buy, execute_sell, prepare_state, get_reward

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ ë° ê°ì²´ ì´ˆê¸°í™”
config = load_config()
upbit_client = UpbitClient(os.getenv("UPBIT_ACCESS_KEY"), os.getenv("UPBIT_SECRET_KEY"))
openai_client = OpenAIClient(os.getenv("OPENAI_API_KEY"))
performance_monitor = PerformanceMonitor()
position_manager = PositionManager(upbit_client)
order_manager = OrderManager(upbit_client, position_manager)
auto_adjuster = AutoAdjustment(config.get('initial_params', {}))
anomaly_detector = AnomalyDetector()
regime_detector = MarketRegimeDetector()
ml_predictor = MLPredictor()
rl_agent = RLAgent(state_size=5, action_size=3)  # 3 actions: buy, sell, hold



# ì „ì—­ ë³€ìˆ˜ë¡œ í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì 
learning_progress = {
    'ml_model': {'accuracy': 0, 'loss': 0},
    'rl_agent': {'epsilon': 1.0, 'average_reward': 0},
    'gpt4_agreement': 0,
    'total_trades': 0,
    'successful_trades': 0
}


def main():
    logger.info("Starting the trading bot")

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    initialize_db()

    # Upbit ì—°ê²° í™•ì¸
    if not upbit_client.check_connection():
        logger.error("Failed to connect to Upbit. Exiting.")
        return

    # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
    logger.info("Starting backtesting...")
    historical_data = upbit_client.get_ohlcv("KRW-BTC", interval="day", count=365)  # 1ë…„ì¹˜ ì¼ë³„ ë°ì´í„°
    backtest_results = run_backtest(historical_data)
    logger.info(f"Backtest results: {backtest_results}")

    # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸° í•™ìŠµ
    ml_predictor.train(historical_data)

    trading_loop()
    # report_performance()

    # ì£¼ê¸°ì  ì‘ì—… ì„¤ì •
    schedule.every(10).minutes.do(trading_loop)
    # schedule.every(10).minutes.do(report_performance)
    schedule.every().day.at("00:00").do(performance_monitor.save_to_file)

    # ë©”ì¸ ë£¨í”„
    logger.info("Starting main loop")
    while True:
        try:
            schedule.run_pending()
            time.sleep(1)
        except Exception as e:
            logger.error(f"Error in main loop: {e}")
            logger.exception("Traceback:")


def trading_loop():
    try:
        logger.info("Starting trading loop")
        multi_timeframe_data = safe_fetch_multi_timeframe_data(upbit_client)

        # ê°€ì¥ ì§§ì€ ì‹œê°„ëŒ€ì˜ ë°ì´í„°ë¥¼ ê¸°ë³¸ ë°ì´í„°ë¡œ ì‚¬ìš©
        data = multi_timeframe_data['short']

        # ë°±í…ŒìŠ¤íŒ… ìˆ˜í–‰ ë° ê²°ê³¼ ì—…ë°ì´íŠ¸
        backtest_results = run_backtest(data)

        # ML ëª¨ë¸ ì¬í•™ìŠµ ë° ì˜ˆì¸¡
        ml_accuracy, ml_loss = ml_predictor.train(data)
        ml_prediction = ml_predictor.predict(data.iloc[-1:])

        # RL ì—ì´ì „íŠ¸ í•™ìŠµ ë° í–‰ë™ ì„ íƒ
        state = prepare_state(data.iloc[-1:])
        rl_action = rl_agent.act(state)

        # ì´ìƒ íƒì§€ ë° ì‹œì¥ ì²´ì œ ë¶„ì„
        anomalies, anomaly_scores = anomaly_detector.detect_anomalies(data)
        current_regime = regime_detector.detect_regime(data)

        # ì´ì „ ê²°ì • í‰ê°€
        average_accuracy = evaluate_previous_decision(upbit_client)

        # GPT-4ë¥¼ ì´ìš©í•œ ë¶„ì„ ë° ì¡°ì–¸ ì–»ê¸°
        gpt4_advice = analyze_data_with_gpt4(
            data, openai_client, config, upbit_client, average_accuracy,
            anomalies, anomaly_scores, current_regime, ml_prediction, rl_action, backtest_results
        )

        # ìµœì¢… ê²°ì • ìƒì„±
        final_decision = generate_final_decision(gpt4_advice, ml_prediction, rl_action, current_regime, backtest_results)

        # ê±°ë˜ ì‹¤í–‰
        if final_decision['decision'] == 'buy':
            execute_buy(upbit_client, final_decision['percentage'], final_decision['target_price'], config)
        elif final_decision['decision'] == 'sell':
            execute_sell(upbit_client, final_decision['percentage'], final_decision['target_price'], config)

        # RL ì—ì´ì „íŠ¸ í•™ìŠµ
        if len(data) > 1:
            next_state = prepare_state(data.iloc[-1:])
            current_portfolio = {
                'btc_balance': upbit_client.get_balance("BTC"),
                'krw_balance': upbit_client.get_balance("KRW")
            }
            reward = calculate_reward(final_decision, data, current_portfolio)
            done = False
            rl_agent.remember(state[0], rl_action, reward, next_state[0], done)
            if len(rl_agent.memory) > rl_agent.batch_size:
                rl_agent.replay(rl_agent.batch_size)

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë¡
        current_price = upbit_client.get_current_price("KRW-BTC")
        balance = upbit_client.get_balance("KRW")
        btc_amount = upbit_client.get_balance("BTC")
        performance_monitor.record(
            final_decision,
            current_price,
            balance,
            btc_amount,
            final_decision.get('param_adjustment', {}),
            final_decision.get('risk_assessment', 'unknown'),
            any(anomalies)
        )

        # GPT-4ì˜ ê²°ì •ì„ final_decisionì— ì¶”ê°€
        final_decision['gpt4_decision'] = gpt4_advice['decision']

        # í•™ìŠµ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        update_learning_progress(
            ml_accuracy,
            ml_loss,
            rl_agent.epsilon,
            rl_agent.get_average_reward(),
            100 if final_decision['decision'] == final_decision['gpt4_decision'] else 0,
            calculate_trade_success(final_decision, data)
        )

        # í•™ìŠµ ì§„í–‰ ìƒí™© ë³´ê³ 
        report_learning_progress()

    except Exception as e:
        logger.error(f"Error in trading loop: {e}")
        logger.exception("Traceback:")


def calculate_trade_success(decision, data):
    if decision['decision'] == 'hold':
        return True  # HOLD ê²°ì •ì€ í•­ìƒ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
    elif decision['decision'] == 'buy':
        return data['close'].iloc[-1] > data['close'].iloc[-2]
    elif decision['decision'] == 'sell':
        return data['close'].iloc[-1] < data['close'].iloc[-2]
    else:
        return False


def calculate_reward(decision: dict, data: pd.DataFrame, current_portfolio: dict) -> float:
    current_price = data['close'].iloc[-1]
    previous_price = data['close'].iloc[-2]
    price_change = (current_price - previous_price) / previous_price

    # ê¸°ë³¸ ë³´ìƒ: ê²°ì •ì˜ ì •í™•ì„±
    if decision['decision'] == 'buy' and price_change > 0:
        base_reward = 1
    elif decision['decision'] == 'sell' and price_change < 0:
        base_reward = 1
    elif decision['decision'] == 'hold' and abs(price_change) < 0.005:  # 0.5% ì´ë‚´ ë³€ë™
        base_reward = 0.5
    else:
        base_reward = -1

    # ìˆ˜ìµë¥  ë³´ìƒ
    profit_reward = 0
    if decision['decision'] == 'buy':
        profit_reward = price_change * decision['percentage'] / 100
    elif decision['decision'] == 'sell':
        profit_reward = -price_change * decision['percentage'] / 100

    # ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë³´ìƒ
    risk_reward = 0
    if 'stop_loss' in decision and 'take_profit' in decision:
        stop_loss_distance = (current_price - decision['stop_loss']) / current_price
        take_profit_distance = (decision['take_profit'] - current_price) / current_price
        if stop_loss_distance > 0 and take_profit_distance > 0:
            risk_reward = min(stop_loss_distance, take_profit_distance)

    # ê±°ë˜ ë¹ˆë„ ì¡°ì ˆ ë³´ìƒ
    frequency_penalty = -0.1 if decision['decision'] != 'hold' else 0

    # í¬íŠ¸í´ë¦¬ì˜¤ ë‹¤ê°í™” ë³´ìƒ
    diversification_reward = 0
    if current_portfolio['btc_balance'] > 0 and current_portfolio['krw_balance'] > 0:
        btc_ratio = current_portfolio['btc_balance'] * current_price / (current_portfolio['btc_balance'] * current_price + current_portfolio['krw_balance'])
        diversification_reward = 1 - abs(0.5 - btc_ratio)  # 50:50 ë¹„ìœ¨ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ë³´ìƒ

    # ì´ ë³´ìƒ ê³„ì‚°
    total_reward = (
        base_reward * 0.4 +
        profit_reward * 2 +
        risk_reward * 0.5 +
        frequency_penalty +
        diversification_reward * 0.3
    )

    # ë³´ìƒ ì •ê·œí™” (-1ì—ì„œ 1 ì‚¬ì´ë¡œ)
    normalized_reward = np.clip(total_reward, -1, 1)

    return normalized_reward


def evaluate_previous_decision(upbit_client):
    previous_decision = get_previous_decision()
    if not previous_decision:
        logger.info("No previous decision found to evaluate.")
        return 0

    actual_price = upbit_client.get_current_price("KRW-BTC")
    accuracy = 0

    if previous_decision['decision'] == 'buy' and actual_price >= previous_decision['target_price']:
        accuracy = 1
    elif previous_decision['decision'] == 'sell' and actual_price <= previous_decision['target_price']:
        accuracy = 1

    update_decision_accuracy(previous_decision['id'], accuracy)
    logger.info(f"Previous decision evaluated: {'Success' if accuracy == 1 else 'Failure'} with accuracy {accuracy}")

    # ìµœê·¼ ê²°ì •ë“¤ì˜ í‰ê·  ì •í™•ë„ ê³„ì‚°
    recent_decisions = get_recent_decisions(days=7)  # ìµœê·¼ 7ì¼ê°„ì˜ ê²°ì • ê°€ì ¸ì˜¤ê¸°
    if recent_decisions:
        average_accuracy = sum(d['accuracy'] for d in recent_decisions if d['accuracy'] is not None) / len(recent_decisions)
    else:
        average_accuracy = 0

    return average_accuracy


def generate_final_decision(gpt4_advice, ml_prediction, rl_action, current_regime, backtest_results):
    decision_weights = {
        'gpt4': 0.4,
        'ml': 0.3,
        'rl': 0.3
    }

    def decision_to_number(decision):
        if isinstance(decision, str):
            return {'buy': 1, 'hold': 0, 'sell': -1}.get(decision.lower(), 0)
        return decision

    gpt4_decision = decision_to_number(gpt4_advice.get('decision', 'hold'))
    ml_decision = 1 if ml_prediction == 1 else -1
    rl_decision = rl_action - 1  # Convert 0, 1, 2 to -1, 0, 1

    weighted_decision = (
        gpt4_decision * decision_weights['gpt4'] +
        ml_decision * decision_weights['ml'] +
        rl_decision * decision_weights['rl']
    )

    if weighted_decision > 0.1:
        final_decision = 'buy'
    elif weighted_decision < -0.1:
        final_decision = 'sell'
    else:
        final_decision = 'hold'

    # ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ê³ ë ¤í•œ ë¦¬ìŠ¤í¬ ì¡°ì •
    if backtest_results.get('sharpe_ratio', 0) < 0.5:
        final_decision = 'hold'  # ë¦¬ìŠ¤í¬ê°€ ë†’ì„ ê²½ìš° í™€ë“œ

    # ì•ˆì „í•˜ê²Œ ê°’ì„ ê°€ì ¸ì˜¤ê³  ê¸°ë³¸ê°’ ì„¤ì •
    def safe_float(value, default=0.0):
        try:
            return float(value) if value is not None else default
        except (ValueError, TypeError):
            return default

    return {
        'decision': final_decision,
        'percentage': min(max(safe_float(gpt4_advice.get('percentage'), 0), 0), 100),
        'target_price': safe_float(gpt4_advice.get('target_price')),
        'stop_loss': safe_float(gpt4_advice.get('stop_loss')),
        'take_profit': safe_float(gpt4_advice.get('take_profit')),
        'reasoning': gpt4_advice.get('reasoning', 'No reasoning provided'),
        'risk_assessment': gpt4_advice.get('risk_assessment', 'unknown')
    }


def report_performance():
    summary = performance_monitor.get_performance_summary()
    accuracy = performance_monitor.get_prediction_accuracy()
    message = f"ğŸ“Š Performance Summary (Last 10 minutes):\n{summary}\n\nPrediction Accuracy: {accuracy:.2f}%"
    # send_discord_message(message)


def update_learning_progress(ml_accuracy, ml_loss, rl_epsilon, rl_reward, gpt4_agreement, trade_success):
    global learning_progress
    learning_progress['ml_model']['accuracy'] = ml_accuracy
    learning_progress['ml_model']['loss'] = ml_loss
    learning_progress['rl_agent']['epsilon'] = rl_epsilon
    learning_progress['rl_agent']['average_reward'] = rl_reward
    learning_progress['gpt4_agreement'] = gpt4_agreement
    learning_progress['total_trades'] += 1
    if trade_success:
        learning_progress['successful_trades'] += 1


def report_learning_progress():
    global learning_progress

    cumulative_summary = performance_monitor.get_cumulative_summary()
    detailed_analysis = performance_monitor.get_detailed_analysis()
    success_rate = (cumulative_summary['successful_trades'] / cumulative_summary['total_trades'] * 100
                    if cumulative_summary['total_trades'] > 0 else 0)
    performance_comparison = performance_monitor.get_performance_comparison()

    report = f"""
    ğŸ“Š íŠ¸ë ˆì´ë”© ì„±ê³¼ ë¹„êµ ğŸ“Š
    íŠ¸ë ˆì´ë”© ìˆ˜ìµë¥ : {performance_comparison['trading_return']:.2f}%
    HODL ìˆ˜ìµë¥ : {performance_comparison['hodl_return']:.2f}%
    ì´ˆê³¼ ì„±ê³¼: {performance_comparison['outperformance']:.2f}%

    ğŸ“ˆ ìƒì„¸ í•™ìŠµ ì§„í–‰ ë³´ê³ ì„œ ğŸ“ˆ

    ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸: ì •í™•ë„: {learning_progress['ml_model']['accuracy']:.2f}% | ì†ì‹¤: {learning_progress['ml_model']['loss']:.4f}
    ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸: ì—¡ì‹¤ë¡ : {learning_progress['rl_agent']['epsilon']:.4f} | í‰ê·  ë³´ìƒ: {learning_progress['rl_agent']['average_reward']:.2f}
    GPT-4 ì¼ì¹˜ìœ¨: {learning_progress['gpt4_agreement']:.2f}%

    ëˆ„ì  íŠ¸ë ˆì´ë”© ì„±ê³¼:
    - ì´ ê±°ë˜ íšŸìˆ˜: {cumulative_summary['total_trades']}íšŒ
    - ë§¤ìˆ˜ ê±°ë˜: {cumulative_summary['buy_trades']}íšŒ
    - ë§¤ë„ ê±°ë˜: {cumulative_summary['sell_trades']}íšŒ
    - í™€ë”© ê²°ì •: {cumulative_summary['hold_decisions']}íšŒ
    - í‰ê·  ê±°ë˜ ê·œëª¨: {cumulative_summary['avg_trade_size']:.2f}%
    - ê°€ê²© ë³€ë™: {cumulative_summary['price_change']:.2f}%
    - ì”ê³  ë³€ë™: {cumulative_summary['balance_change']:.2f}%
    - ì„±ê³µë¥ : {success_rate:.2f}%

    ì˜ˆì¸¡ ë¶„ì„:
    ìƒìŠ¹ ì˜ˆì¸¡ (ë§¤ìˆ˜/BTC ë³´ìœ  ì‹œ í™€ë”©): ì´ : {detailed_analysis['up_predictions']['total']}íšŒ | ì„±ê³µ: {detailed_analysis['up_predictions']['successful']}íšŒ | ì •í™•ë„: {detailed_analysis['up_predictions']['accuracy']:.2f}%
    í•˜ë½ ì˜ˆì¸¡ (ë§¤ë„/BTC ë¯¸ë³´ìœ  ì‹œ í™€ë”©): ì´ : {detailed_analysis['down_predictions']['total']}íšŒ | ì„±ê³µ: {detailed_analysis['down_predictions']['successful']}íšŒ | ì •í™•ë„: {detailed_analysis['down_predictions']['accuracy']:.2f}%
    ì „ì²´ ì˜ˆì¸¡ ì •í™•ë„: {detailed_analysis['overall_accuracy']:.2f}%
    ì‹¤íŒ¨ ë¶„ì„: ì›ì¸: {detailed_analysis['failure_reasons']}| ê°œì„ : {detailed_analysis['improvement_suggestions']}

    ê³„ì†í•´ì„œ í•™ìŠµí•˜ê³  ê°œì„ í•´ ë‚˜ê°€ê² ìŠµë‹ˆë‹¤! ğŸš€
    """

    send_discord_message(report)





if __name__ == "__main__":
    main()