import logging
import os
import time
from typing import Dict, Any

import pandas as pd
import schedule
from dotenv import load_dotenv

from api_client import UpbitClient, OpenAIClient, OrderManager, PositionManager
from auto_adjustment import AutoAdjustment, AnomalyDetector, MarketRegimeDetector
from backtesting import run_backtest
from config import load_config, setup_logging
from data_preparation import safe_fetch_multi_timeframe_data
from database import initialize_db, get_previous_decision, update_decision_accuracy, get_recent_decisions
from discord_notifier import send_discord_message
from ml_models import MLPredictor, RLAgent
from performance_monitor import PerformanceMonitor
from trading_logic import analyze_data_with_gpt4, execute_buy, execute_sell, prepare_state, get_reward

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ ë° ê°ì²´ ì´ˆê¸°í™”
config = load_config()
upbit_client = UpbitClient(os.getenv("UPBIT_ACCESS_KEY"), os.getenv("UPBIT_SECRET_KEY"))
openai_client = OpenAIClient(os.getenv("OPENAI_API_KEY"))
performance_monitor = PerformanceMonitor()
position_manager = PositionManager(upbit_client)
order_manager = OrderManager(upbit_client, position_manager)
auto_adjuster = AutoAdjustment(config.get('initial_params', {}))
anomaly_detector = AnomalyDetector()
regime_detector = MarketRegimeDetector()
ml_predictor = MLPredictor()
rl_agent = RLAgent(state_size=5, action_size=3)  # 3 actions: buy, sell, hold


def main():
    logger.info("Starting the trading bot")

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    initialize_db()

    # Upbit ì—°ê²° í™•ì¸
    if not upbit_client.check_connection():
        logger.error("Failed to connect to Upbit. Exiting.")
        return

    # ë°±í…ŒìŠ¤íŒ… ì‹¤í–‰
    logger.info("Starting backtesting...")
    historical_data = upbit_client.get_ohlcv("KRW-BTC", interval="day", count=365)  # 1ë…„ì¹˜ ì¼ë³„ ë°ì´í„°
    backtest_results = run_backtest(historical_data)
    logger.info(f"Backtest results: {backtest_results}")

    # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì´ˆê¸° í•™ìŠµ
    ml_predictor.train(historical_data)

    trading_loop()

    # ì£¼ê¸°ì  ìž‘ì—… ì„¤ì •
    schedule.every(10).minutes.do(trading_loop)
    schedule.every(10).minutes.do(report_performance)
    schedule.every().day.at("00:00").do(performance_monitor.save_to_file)

    # ë©”ì¸ ë£¨í”„
    logger.info("Starting main loop")
    while True:
        try:
            schedule.run_pending()
            time.sleep(1)
        except Exception as e:
            logger.error(f"Error in main loop: {e}")
            logger.exception("Traceback:")


def trading_loop():
    try:
        logger.info("Starting trading loop")
        multi_timeframe_data = safe_fetch_multi_timeframe_data(upbit_client)

        # ê°€ìž¥ ì§§ì€ ì‹œê°„ëŒ€ì˜ ë°ì´í„°ë¥¼ ê¸°ë³¸ ë°ì´í„°ë¡œ ì‚¬ìš©
        data = multi_timeframe_data['short']

        # ë°±í…ŒìŠ¤íŒ… ìˆ˜í–‰ ë° ê²°ê³¼ ì—…ë°ì´íŠ¸
        backtest_results = run_backtest(data)

        # ML ëª¨ë¸ ìž¬í•™ìŠµ ë° ì˜ˆì¸¡
        ml_accuracy, ml_loss = ml_predictor.train(data)
        ml_prediction = ml_predictor.predict(data.iloc[-1:])

        # RL ì—ì´ì „íŠ¸ í•™ìŠµ ë° í–‰ë™ ì„ íƒ
        state = prepare_state(data.iloc[-1:])
        rl_action = rl_agent.act(state)

        # íŠ¸ë ˆì´ë”© ì „ëžµ ì‹¤í–‰ ë° ê²°ì • ìƒì„±
        decision = execute_trading_strategy(
            data, upbit_client, config, auto_adjuster, anomaly_detector,
            regime_detector, backtest_results, ml_prediction, rl_action
        )
        logger.info(f"Trading decision: {decision}")

        # GPT-4ì™€ì˜ ì¼ì¹˜ë„ ê³„ì‚°
        gpt4_agreement = 100 if decision['decision'] == decision.get('gpt4_decision', '') else 0

        # ê±°ëž˜ ì„±ê³µ ì—¬ë¶€ (ê°„ë‹¨í•œ ì˜ˆì‹œ, ì‹¤ì œë¡œëŠ” ë” ë³µìž¡í•œ ë¡œì§ì´ í•„ìš”í•  ìˆ˜ ìžˆìŒ)
        trade_success = decision['decision'] == 'hold' or (
                    decision['decision'] == 'buy' and data['close'].iloc[-1] > data['close'].iloc[-2]) or (
                                    decision['decision'] == 'sell' and data['close'].iloc[-1] < data['close'].iloc[-2])

        # í•™ìŠµ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        update_learning_progress(
            ml_accuracy,
            ml_loss,
            rl_agent.epsilon,
            rl_agent.get_average_reward(),
            gpt4_agreement,
            trade_success
        )

        # í•™ìŠµ ì§„í–‰ ìƒí™© ë³´ê³ 
        report_learning_progress()



        # RL ì—ì´ì „íŠ¸ í•™ìŠµ
        if len(data) > 1:
            next_state = prepare_state(data.iloc[-1:])
            reward = calculate_reward(decision, data)
            done = False
            rl_agent.remember(state[0], rl_action, reward, next_state[0], done)
            if len(rl_agent.memory) > rl_agent.batch_size:
                rl_agent.replay(rl_agent.batch_size)

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë¡
        current_price = upbit_client.get_current_price("KRW-BTC")
        balance = upbit_client.get_balance("KRW")
        btc_amount = upbit_client.get_balance("BTC")
        performance_monitor.record(
            decision,
            current_price,
            balance,
            btc_amount,
            decision.get('param_adjustment', {}),
            decision.get('risk_assessment', 'unknown'),
            decision.get('anomalies_detected', False)
        )

    except Exception as e:
        logger.error(f"Error in trading loop: {e}")
        logger.exception("Traceback:")


def execute_trading_strategy(
        data: pd.DataFrame,
        upbit_client: UpbitClient,
        config: Dict[str, Any],
        auto_adjuster: AutoAdjustment,
        anomaly_detector: AnomalyDetector,
        regime_detector: MarketRegimeDetector,
        backtest_results: Dict[str, Any],
        ml_prediction: int,
        rl_action: int
) -> Dict[str, Any]:
    """Execute the trading strategy and return a trading decision."""

    # ì´ì „ ê²°ì • í‰ê°€
    evaluate_previous_decision(upbit_client)

    # ìµœê·¼ ì •í™•ë„ë¥¼ ë°˜ì˜í•œ ë°ì´í„° ë¶„ì„
    average_accuracy = analyze_database_for_decision()

    # í˜„ìž¬ ì‹œìž¥ ìƒíƒœ ë¶„ì„
    anomalies, anomaly_scores = anomaly_detector.detect_anomalies(data)
    current_regime = regime_detector.detect_regime(data)
    current_params = auto_adjuster.adjust_params()

    # GPT-4ë¥¼ ì´ìš©í•œ ë¶„ì„ ë° ì¡°ì–¸ ì–»ê¸°
    gpt4_advice = analyze_data_with_gpt4(
        data, openai_client, current_params, upbit_client, average_accuracy,
        anomalies, anomaly_scores, current_regime, ml_prediction, rl_action, backtest_results
    )

    # ìµœì¢… ê²°ì • ìƒì„±
    final_decision = generate_final_decision(gpt4_advice, ml_prediction, rl_action, current_regime, backtest_results)

    # ê±°ëž˜ ì‹¤í–‰
    if final_decision['decision'] == 'buy':
        execute_buy(upbit_client, final_decision['percentage'], final_decision['target_price'], config)
    elif final_decision['decision'] == 'sell':
        execute_sell(upbit_client, final_decision['percentage'], final_decision['target_price'], config)

    # ë°˜í™˜ê°’ì— anomalies ì •ë³´ ì¶”ê°€
    final_decision['anomalies_detected'] = any(anomalies)
    # GPT-4ì˜ ê²°ì •ì„ final_decisionì— ì¶”ê°€
    final_decision['gpt4_decision'] = gpt4_advice['decision']
    return final_decision


def generate_final_decision(gpt4_advice, ml_prediction, rl_action, current_regime, backtest_results):
    """Generate the final trading decision based on all available information."""
    decision_weights = {
        'gpt4': 0.4,
        'ml': 0.3,
        'rl': 0.3
    }

    def decision_to_number(decision):
        if isinstance(decision, str):
            return {'buy': 1, 'hold': 0, 'sell': -1}.get(decision.lower(), 0)
        return decision

    gpt4_decision = decision_to_number(gpt4_advice['decision'])
    ml_decision = 1 if ml_prediction == 1 else -1
    rl_decision = rl_action - 1  # Convert 0, 1, 2 to -1, 0, 1

    weighted_decision = (
        gpt4_decision * decision_weights['gpt4'] +
        ml_decision * decision_weights['ml'] +
        rl_decision * decision_weights['rl']
    )

    if weighted_decision > 0.1:
        final_decision = 'buy'
    elif weighted_decision < -0.1:
        final_decision = 'sell'
    else:
        final_decision = 'hold'

    # ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ê³ ë ¤í•œ ë¦¬ìŠ¤í¬ ì¡°ì •
    if backtest_results['sharpe_ratio'] < 0.5:
        final_decision = 'hold'  # ë¦¬ìŠ¤í¬ê°€ ë†’ì„ ê²½ìš° í™€ë“œ

    return {
        'decision': final_decision,
        'percentage': gpt4_advice['percentage'],
        'target_price': gpt4_advice['target_price'],
        'stop_loss': gpt4_advice['stop_loss'],
        'take_profit': gpt4_advice['take_profit'],
        'reasoning': f"GPT-4: {gpt4_advice['decision']}, ML: {ml_prediction}, RL: {rl_action}, Regime: {current_regime}",
        'risk_assessment': gpt4_advice['risk_assessment']
    }


def calculate_reward(decision, data):
    if decision['decision'] == 'buy':
        return (data['close'].iloc[-1] - data['close'].iloc[-2]) / data['close'].iloc[-2]
    elif decision['decision'] == 'sell':
        return (data['close'].iloc[-2] - data['close'].iloc[-1]) / data['close'].iloc[-2]
    else:
        return 0


def report_performance():
    summary = performance_monitor.get_performance_summary()
    accuracy = performance_monitor.get_prediction_accuracy()
    message = f"ðŸ“Š Performance Summary (Last 10 minutes):\n{summary}\n\nPrediction Accuracy: {accuracy:.2f}%"
    send_discord_message(message)


def evaluate_previous_decision(upbit_client: UpbitClient):
    """ì´ì „ ê²°ì •ê³¼ ì‹¤ì œ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ìž¥."""
    previous_decision = get_previous_decision()
    if not previous_decision:
        logger.info("No previous decision found to evaluate.")
        return

    actual_price = upbit_client.get_current_price("KRW-BTC")
    accuracy = 0

    if previous_decision['decision'] == 'buy' and actual_price >= previous_decision['target_price']:
        accuracy = 1
    elif previous_decision['decision'] == 'sell' and actual_price <= previous_decision['target_price']:
        accuracy = 1

    update_decision_accuracy(previous_decision['id'], accuracy)
    logger.info(f"Previous decision evaluated: {'Success' if accuracy == 1 else 'Failure'} with accuracy {accuracy}")

    return accuracy


def analyze_database_for_decision():
    """ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ìµœê·¼ ê¸°ë¡ì„ ë” ë¹„ì¤‘ ìžˆê²Œ ë°˜ì˜."""
    recent_decisions = get_recent_decisions(days=30)
    total_accuracy = 0
    count = 0

    for decision in recent_decisions:
        if decision['accuracy'] is not None:
            total_accuracy += decision['accuracy']
            count += 1

    average_accuracy = total_accuracy / count if count > 0 else 0
    logger.info(f"Average accuracy over the last 30 days: {average_accuracy:.2f}")

    return average_accuracy


# ì „ì—­ ë³€ìˆ˜ë¡œ í•™ìŠµ ì§„í–‰ ìƒí™© ì¶”ì 
learning_progress = {
    'ml_model': {'accuracy': 0, 'loss': 0},
    'rl_agent': {'epsilon': 1.0, 'average_reward': 0},
    'gpt4_agreement': 0,
    'total_trades': 0,
    'successful_trades': 0
}


def update_learning_progress(ml_accuracy, ml_loss, rl_epsilon, rl_reward, gpt4_agreement, trade_success):
    global learning_progress
    learning_progress['ml_model']['accuracy'] = ml_accuracy
    learning_progress['ml_model']['loss'] = ml_loss
    learning_progress['rl_agent']['epsilon'] = rl_epsilon
    learning_progress['rl_agent']['average_reward'] = rl_reward
    learning_progress['gpt4_agreement'] = gpt4_agreement
    learning_progress['total_trades'] += 1
    if trade_success:
        learning_progress['successful_trades'] += 1


def report_learning_progress():
    global learning_progress

    success_rate = (learning_progress['successful_trades'] / learning_progress['total_trades'] * 100
                    if learning_progress['total_trades'] > 0 else 0)

    report = f"""
    ðŸ“Š Learning Progress Report ðŸ“Š
    
    ML Model:
    - Accuracy: {learning_progress['ml_model']['accuracy']:.2f}%
    - Loss: {learning_progress['ml_model']['loss']:.4f}
    
    RL Agent:
    - Epsilon: {learning_progress['rl_agent']['epsilon']:.4f}
    - Average Reward: {learning_progress['rl_agent']['average_reward']:.2f}
    
    GPT-4 Agreement Rate: {learning_progress['gpt4_agreement']:.2f}%
    
    Trading Performance:
    - Total Trades: {learning_progress['total_trades']}
    - Successful Trades: {learning_progress['successful_trades']}
    - Success Rate: {success_rate:.2f}%
    
    Keep learning and improving! ðŸš€
    """

    send_discord_message(report)



if __name__ == "__main__":
    main()